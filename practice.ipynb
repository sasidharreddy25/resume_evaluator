{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from utils import Resume_Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_evaluator=Resume_Evaluator('azure_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sort=resume_evaluator.evaluate('Document10.pdf', 'resumes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sashidhar_GenAI.pdf': [{'strengths': ['2+ years of experience in implementing and deploying machine learning and deep learning frameworks on cloud platforms like AWS and GCP',\n",
       "    '1+ years of professional experience with NLP and Large Language Models (LLM)',\n",
       "    'Proficiency in Python programming, with working experience over one of the ML Frameworks - SKLearn, XGBoost, PyCaret, and Deep Learning Frameworks - Tensorflow, PyTorch, Huggingface, and Rasa'],\n",
       "   'gaps': ['Bachelor’s Degree (might be final course student) in Statistics, Applied Mathematics, Computer Science, or other related fields',\n",
       "    'LLM Model Fine-Tuning: Utilize your expertise in encoding and language understanding to fine-tune existing Large Language Models for optimal performance in solving domain-specific needs.',\n",
       "    'Model Training and Evaluation: Conduct rigorous testing and evaluation of LLMs to identify areas for improvement and implement solutions to enhance their performance.'],\n",
       "   'questions_to_candidate': [\"Do you have a Bachelor's Degree in Statistics, Applied Mathematics, Computer Science, or other related fields?\",\n",
       "    'Can you provide examples of your experience in LLM Model Fine-Tuning and Model Training and Evaluation?',\n",
       "    'Have you worked on any projects involving the end-to-end deployment of LLMs?'],\n",
       "   'candidate_name': ['Sashidhar Reddy Duddukunta']},\n",
       "  {'final_score': 0.5476190476190477}],\n",
       " 'RaviKumarCV.pdf': [{'strengths': ['Proficiency in Python programming, with working experience over one of the ML Frameworks - PyTorch',\n",
       "    '2+ years of experience in implementing and deploying machine learning and deep learning frameworks on cloud platforms like AWS',\n",
       "    'Experience working with End to End Machine Learning project Life cycle'],\n",
       "   'gaps': ['1+ years of professional experience with NLP and Large Language Models (LLM)',\n",
       "    'Experience with LLM/GenAI frameworks and tools',\n",
       "    'LLM Model Fine-Tuning expertise'],\n",
       "   'questions_to_candidate': ['Do you have any professional experience with NLP and Large Language Models (LLM)?',\n",
       "    'Have you worked with LLM/GenAI frameworks and tools?',\n",
       "    'Can you provide examples of your LLM Model Fine-Tuning expertise?'],\n",
       "   'candidate_name': ['Ravi Kumar']},\n",
       "  {'final_score': 0.47619047619047616}],\n",
       " 'Arushi_Gogia__-_Data Scientist_240730_221057.pdf': [{'strengths': ['1+ years of professional experience with NLP and Large Language Models (LLM)',\n",
       "    '2+ years of experience in implementing and deploying machine learning and deep learning frameworks on cloud platforms like AWS and GCP',\n",
       "    'Proficiency in Python programming, with working experience over one of the ML Frameworks - SKLearn, XGBoost, PyCaret, and Deep Learning Frameworks - Tensorflow, PyTorch, Huggingface, and Rasa'],\n",
       "   'gaps': ['Bachelor’s Degree (might be final course student) in Statistics, Applied Mathematics, Computer Science, or other related fields',\n",
       "    'LLM Model Fine-Tuning: Utilize your expertise in encoding and language understanding to fine-tune existing Large Language Models for optimal performance in solving domain-specific needs',\n",
       "    'Model Training and Evaluation: Conduct rigorous testing and evaluation of LLMs to identify areas for improvement and implement solutions to enhance their performance'],\n",
       "   'questions_to_candidate': [\"Do you have a Bachelor's Degree in Statistics, Applied Mathematics, Computer Science, or other related fields?\",\n",
       "    'Can you provide examples of your experience in LLM Model Fine-Tuning and Model Training and Evaluation?',\n",
       "    'Have you worked on any projects involving the deployment of Large Language Models?'],\n",
       "   'candidate_name': ['Arushi Gogia']},\n",
       "  {'final_score': 0.4523809523809524}],\n",
       " 'Sashidhar_Resume.pdf': [{'strengths': ['Proficiency in Python programming',\n",
       "    'Interest in NLP and ML',\n",
       "    'Experience in implementing ML algorithms'],\n",
       "   'gaps': ['No professional experience with NLP and Large Language Models (LLM)',\n",
       "    'No experience in deploying machine learning and deep learning frameworks on cloud platforms',\n",
       "    'No mention of working with LLM/GenAI frameworks and tools'],\n",
       "   'questions_to_candidate': ['Do you have any professional experience with NLP and Large Language Models (LLM)?',\n",
       "    'Have you worked with machine learning and deep learning frameworks on cloud platforms like AWS and GCP?',\n",
       "    'Have you worked with LLM/GenAI frameworks and tools?'],\n",
       "   'candidate_name': ['Sashidhar Reddy Duddukunta']},\n",
       "  {'final_score': 0.3333333333333333}],\n",
       " 'Shashank_Resume_1.pdf': [{'strengths': ['1+ years of professional experience with NLP and Large Language Models (LLM)',\n",
       "    'Experience working with End to End Machine Learning project Life cycle',\n",
       "    'Proficiency in Python programming, with working experience over one of the ML Frameworks - SKLearn, XGBoost, PyCaret, and Deep Learning Frameworks - Tensorflow, PyTorch, Huggingface, and Rasa'],\n",
       "   'gaps': ['Bachelor’s Degree (might be final course student) in Statistics, Applied Mathematics, Computer Science, or other related fields',\n",
       "    '2+ years of experience in implementing and deploying machine learning and deep learning frameworks on cloud platforms like AWS and GCP.'],\n",
       "   'questions_to_candidate': [\"Do you have a Bachelor's Degree in Statistics, Applied Mathematics, Computer Science, or other related fields?\",\n",
       "    'Do you have 2+ years of experience in implementing and deploying machine learning and deep learning frameworks on cloud platforms like AWS and GCP?',\n",
       "    'Can you provide more details about your experience with End to End Machine Learning project Life cycle?'],\n",
       "   'candidate_name': ['Shashank Gupta']},\n",
       "  {'final_score': 0.2857142857142857}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import PyPDF2\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_engine_name=\"gpt-35-turbo-16k\"\n",
    "embedding_model_name=\"text-embedding-002\"\n",
    "azure_endpoint = \"https://securityappdewa.openai.azure.com\"\n",
    "api_key=\"576b185ff87d4f50962560d349d06332\"\n",
    "api_version= \"2023-07-01-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = AzureOpenAI(\n",
    "                azure_endpoint = azure_endpoint,\n",
    "                api_key=api_key,\n",
    "                api_version= api_version\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text() if page.extract_text() else \"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "l=os.listdir(\"resumes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resumes/sashidhar_GenAI.pdf\n"
     ]
    }
   ],
   "source": [
    "for i in l:\n",
    "    if i==\"sashidhar_GenAI.pdf\":\n",
    "        print(\"resumes\"+\"/\"+\"sashidhar_GenAI.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text=extract_text_from_pdf('resumes\\RaviKumarCV.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description=extract_text_from_pdf('Document10.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_agent(jd):\n",
    "        res = openai_client.chat.completions.create(model=gpt_engine_name,\n",
    "                                                        messages=[\n",
    "                                                                {'role': 'system','content':f'''As a job recruiter agent, your task is to extract the required details mentioned in the given job description and output them as a JSON response.\n",
    "                                                                \n",
    "Step 1: Analyze the job description provided below to extract precise details.\n",
    "Job Description:\n",
    "```{jd}``` \n",
    "\n",
    "Step 2: Based on the analysis in step 1, extract the 'educational degree' that a candidate must have as mentioned in the job description. Provide the precise education details specified. If there are multiple optional degrees, list them in the same string separated by \"or.\" parameter. Output it as a list. If the educational requirement is not mentioned in the job description, provide an empty list.                                                                                                                                                                                                                                 \n",
    "                                                        \n",
    "Step 3: Based on the analysis in step 1, extract the both technical, soft 'skills' that the recruiter is seeking in a candidate from the 'job description'. List each skill 'individually and distinctly', without grouping multiple skills together. Exhaustively extract all the skills mentioned in the job description as 'individual entities'.\n",
    "\n",
    "Step 4: Based on the analysis in step 1, extract the level of experience that the recruiter is looking for in a candidate from the job description. List the experience level(s) in a list. If the experience level is not mentioned in the job description, then it shoukd be empty list.\n",
    "The skills are very important, so please extract them exhaustively, including those mentioned implicitly.\n",
    "\n",
    "Step 5: Based on the analysis in step 1, extract whether the recruiter is looking for a candidate with experience in a specific industry or possessing particular domain knowledge. If no such details are mentioned in the job description, provide an empty list.\n",
    "\n",
    "Step 5: Based on the analysis in step 1, Extract all other requirements mentioned in the job description that do not fall under the categories of Educational Qualifications, Skills, Experience Level, or Industry and Domain Knowledge. Provide them in a list.\n",
    "\n",
    "Step 6: Output a JSON Response with the following keys: \"Educational Qualifications,\" \"Skills,\" \"Experience Level,\", \"Industry and Domain Knowledge.\" and \"others\". The values for each key should be lists of the respective extracted details. Ensure that the extracted values are not repeated across different lists.\n",
    "\n",
    "Extract as many requirements as possible from the given job description into their respective categories.\n",
    "'''},  \n",
    "                                                                {'role': 'user',  'content':'''Based on the give above instructions extract the requirements from the job descripion. When extracting the skills, 'ensure each skill is listed as an individual entity' in the list. \"If the requirements are separated by 'or, /' indicating that any one of them is acceptable to the recruiter, list them as a single entity in a string.\". Extract as many requirements as possible.\n",
    "Be precise and thoroughly extract all the values from the job description. Output the Json Response with keys and list of values. Be precise with the requirements; avoid providing any vague or generic details.\n",
    "Json Response:'''}]\n",
    "                                                        )\n",
    "        requirements=json.loads(res.choices[0].message.content)\n",
    "\n",
    "        return requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements=extract_agent(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_requires=[]\n",
    "for i, j in requirements.items():\n",
    "    job_requires.extend(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recruiter_agent(messages):\n",
    "    res = openai_client.chat.completions.create(model=gpt_engine_name,\n",
    "                                                messages=messages,\n",
    "                                                temperature=0)\n",
    "    return res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_agent(messages):\n",
    "    res = openai_client.chat.completions.create(model=gpt_engine_name,\n",
    "                                                messages=messages,\n",
    "                                                max_tokens=200,\n",
    "                                                temperature=0)\n",
    "    return res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_agent(messages):\n",
    "    res = openai_client.chat.completions.create(model=gpt_engine_name,\n",
    "                                                messages=messages,\n",
    "                                                temperature=0)\n",
    "    score=json.loads(res.choices[0].message.content)\n",
    "    return score['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruiter_system='''Your are a recruiter look for a best candidate for the vacant position. Your task is to ask the candidate question based on the given requirement below for job.\n",
    "\n",
    "Job Requirement:\n",
    "```\n",
    "{job_requirement}\n",
    "```\n",
    "The above is the job requirement for the vacant posiiton, so please ask the candidate a question on this requirement. Ask the question directly without introducing yourself or adding extra information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_system=f'''You are a candidate looking for a job. You only have the expertise, skills, and experience mentioned in the resume below. Apart from the information provided in the resume, you have no additional knowledge or experience. Your resume is mentioned below in triple backticks(```).\n",
    "\n",
    "Step 1: Analyse your resume given below.\n",
    "Resume:\n",
    "```{resume_text}```\n",
    "\n",
    "Step 2: Based on the analysis in Step 1, you should respond to the recruiter's questions solely using the information in your resume. Analyze the recruiter's questions carefully and base your answers strictly on the details provided in your resume.\n",
    "You 'should not lie' about anything to the recruiter, as lying will severely impact your chances of being selected for the job. If you lie, you will be disqualified from the recruitment process. If you do not know answer to the recruiter question then simply say \"sorry, I do not know answer to your question\".\n",
    "When responding to the recruiter's questions, be precise and answer based solely on the information provided in the 'resume'. Avoid making \"assumptions or adding any information that isn't explicitly mentioned in the resume\" in your answer\n",
    "\n",
    "Step 3: Ensure that the answer is precise and 'based on the resume'. Ensure your answer is strictly under 200 words. Base your response solely on information from the resume, without including any external knowledge.\n",
    "Your answer should be \"precise\", using only the terms and wording specified in the resume. \"Do not include anything not mentioned in the resume when communicating with the recruiter\".\n",
    "\n",
    "Step 4: Ensure that the \"answer is exclusively based on the resume\", with 'every word sourced directly' from it. The answer should focus solely on the topic asked by the recruiter, without discussing anything else.\n",
    "\n",
    "Follow the above steps and give \"precise answer\", to the point answer to recruiter question.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate='''In my role as a Senior Data Scientist at Celebal Technologies, I have gained extensive experience in supporting all stages of the ML model life cycle. I have worked on projects involving AI solutions, prompt optimization, and prompt engineering using LLM models and Python programming.\n",
    "\n",
    "During the development phase, I have been responsible for robust data extraction from various file types, such as PDFs, Word documents, Excel spreadsheets, and more. I have utilized entity recognition, summarization, and question-answering techniques using Azure OpenAI LLM models. Additionally, I have applied prompt engineering methodologies like CoT and ReACT, along with frameworks like CO-STAR, to ensure accuracy and efficiency.\n",
    "\n",
    "In the deployment phase, I have contributed to the enhancement of LangChain LLM Agents, leveraging advanced NLP techniques for language understanding and generation. I have also ensured the accuracy of generated responses by leveraging the RAGAS Library.\n",
    "\n",
    "Throughout the ML model life cycle, I have focused on optimizing performance and user experiences. I have implemented security measures, integrated notification systems, and introduced interactive features to enhance user engagement. Furthermore, I have collaborated closely with stakeholders to gather requirements and deliver context-aware answers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system=f'''You are an job evaluator agent. You will be given job requirement and candidate answer for the requirement. Your task is to analyze the candidate's answer based on the job requirement and determine how well it meets them. Assign a score between 0 and 1 to the candidate's answer, where 0 means no match, 1 means a perfect match, and any value between 0 and 1 indicates a partial match.\n",
    "Do not be generic in your evaluation. Thoroughly analyze how the candidate's answer meets the job requirements and provide the most accurate score. Only award a full score if the candidate's answer completely satisfies the job requirements in every aspect. Do not give a full score easily.\n",
    "If the job requirement is experience with the project life cycle, only assign full marks if the candidate's answer covers all aspects of the project life cycle—requirement gathering, planning, design, development, testing, deployment, and maintenance. If only few are mentioned, then assign \"partial marks\".\n",
    "\n",
    "Step 1: Analyse the given job requirement and candidate answer.\n",
    "job requirement: \n",
    "```{job_requiremet}```\n",
    "\n",
    "Candidate answer: \n",
    "```{candidate}```\n",
    "\n",
    "Step 3: Verify whether the candidate's response to the job requirement is accurate and aligns with the job requirements. If the candidate's answer thoroughly and explicitly satisfies all the job requirements, assign a score of 1. If the candidate's response does not meet the job requirements, assign a score of 0.\n",
    "The score ranges from 0 to 1. If the candidate's answer only partially matches the job requirements, assign a 'score between 0 and 1' based on the level of matching. \"only in rare cases where answer matches all requirements you should assign a score of 1\". You should assign 'partial marks' in the case of partial requirements matching.\n",
    "If the candidate states they do not have expertise in the job requirement, assign a score of 0.\n",
    "\n",
    "Step 4: Output a Json Response with score as key and respective score as value. Do not output anything extra information, just give score.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an job evaluator agent. You will be given job requirement and candidate answer for the requirement. Your task is to analyze the candidate's answer based on the job requirement and determine how well it meets them. Assign a score between 0 and 1 to the candidate's answer, where 0 means no match, 1 means a perfect match, and any value between 0 and 1 indicates a partial match.\n",
      "Do not be generic in your evaluation. Thoroughly analyze how the candidate's answer meets the job requirements and provide the most accurate score. Only award a full score if the candidate's answer completely satisfies the job requirements in every aspect. Do not give a full score easily.\n",
      "If the job requirement is experience with the project life cycle, only assign full marks if the candidate's answer covers all aspects of the project life cycle—requirement gathering, planning, design, development, testing, deployment, and maintenance. If only few are mentioned, then assign \"partial marks\".\n",
      "\n",
      "Step 1: Analyse the given job requirement and candidate answer.\n",
      "job requirement: \n",
      "```Ensure support and all stages of ML model life cycle```\n",
      "\n",
      "Candidate answer: \n",
      "```In my role as a Senior Data Scientist at Celebal Technologies, I have gained extensive experience in supporting all stages of the ML model life cycle. I have worked on projects involving AI solutions, prompt optimization, and prompt engineering using LLM models and Python programming.\n",
      "\n",
      "During the development phase, I have been responsible for robust data extraction from various file types, such as PDFs, Word documents, Excel spreadsheets, and more. I have utilized entity recognition, summarization, and question-answering techniques using Azure OpenAI LLM models. Additionally, I have applied prompt engineering methodologies like CoT and ReACT, along with frameworks like CO-STAR, to ensure accuracy and efficiency.\n",
      "\n",
      "In the deployment phase, I have contributed to the enhancement of LangChain LLM Agents, leveraging advanced NLP techniques for language understanding and generation. I have also ensured the accuracy of generated responses by leveraging the RAGAS Library.\n",
      "\n",
      "Throughout the ML model life cycle, I have focused on optimizing performance and user experiences. I have implemented security measures, integrated notification systems, and introduced interactive features to enhance user engagement. Furthermore, I have collaborated closely with stakeholders to gather requirements and deliver context-aware answers.\n",
      "```\n",
      "\n",
      "Step 3: Verify whether the candidate's response to the job requirement is accurate and aligns with the job requirements. If the candidate's answer thoroughly and explicitly satisfies all the job requirements, assign a score of 1. If the candidate's response does not meet the job requirements, assign a score of 0.\n",
      "The score ranges from 0 to 1. If the candidate's answer only partially matches the job requirements, assign a 'score between 0 and 1' based on the level of matching. \"only in rare cases where answer matches all requirements you should assign a score of 1\". You should assign 'partial marks' in the case of partial requirements matching.\n",
      "\n",
      "Step 4: Output a Json Response with score as key and respective score as value. Do not output anything extra information, just give score.\n"
     ]
    }
   ],
   "source": [
    "print(evaluator_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_agent([{'role': 'system','content':evaluator_system}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "recriter_messages=[{'role': 'system','content':recruiter_system}]\n",
    "candidate_messages=[{'role': 'system','content':candidate_system}]\n",
    "evaluation_messages=[{'role': 'system','content':evaluator_system}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(job_requires, resume_text):\n",
    "    QnA=[]\n",
    "    scores=0\n",
    "    for i in job_requires:\n",
    "        recruiter_system='''Your are a recruiter look for a best candidate for the vacant position. Your task is to ask the candidate question based on the given requirement below for job.\n",
    "\n",
    "Job Requirement:\n",
    "```\n",
    "{job_requirement}\n",
    "```\n",
    "The above is the job requirement for the vacant posiiton, so please ask the candidate a question on this requirement. Ask the question directly without introducing yourself or adding extra information.\n",
    "'''\n",
    "        evaluator_system='''You are an job evaluator agent. You will be given job requirement and candidate answer for the requirement. Your task is to analyze the candidate's answer based on the job requirement and determine how well it meets them. Assign a score between 0 and 1 to the candidate's answer, where 0 means no match, 1 means a perfect match, and any value between 0 and 1 indicates a partial match.\n",
    "Do not be generic in your evaluation. Thoroughly analyze how the candidate's answer meets the job requirements and provide the most accurate score. Only award a full score if the candidate's answer completely satisfies the job requirements in every aspect. Do not give a full score easily.\n",
    "If the job requirement is experience with the project life cycle, only assign full marks if the candidate's answer covers all aspects of the project life cycle—requirement gathering, planning, design, development, testing, deployment, and maintenance. If only few are mentioned, then assign \"partial marks\".\n",
    "\n",
    "Step 1: Analyse the given job requirement and candidate answer.\n",
    "job requirement: \n",
    "```{job_requiremet}```\n",
    "\n",
    "Candidate answer: \n",
    "```{candidate}```\n",
    "\n",
    "Step 3: Verify whether the candidate's response to the job requirement is accurate and aligns with the job requirements. If the candidate's answer thoroughly and explicitly satisfies all the job requirements, assign a score of 1. If the candidate's response does not meet the job requirements, assign a score of 0.\n",
    "The score ranges from 0 to 1. If the candidate's answer only partially matches the job requirements, assign a 'score between 0 and 1' based on the level of matching. \"only in rare cases where answer matches all requirements you should assign a score of 1\". You should assign 'partial marks' in the case of partial requirements matching.\n",
    "If the candidate states they do not have expertise in the job requirement, assign a score of 0.\n",
    "\n",
    "Step 4: Output a Json Response with score as key and respective score as value. Do not output anything extra information, just give score.\n",
    "'''\n",
    "        recruiter_system=recruiter_system.format(job_requirement=i)\n",
    "        recriter_messages[0]={'role': 'system','content':recruiter_system}\n",
    "        recruiter=recruiter_agent(recriter_messages)\n",
    "        candidate_messages.append({'role': 'user',  'content':recruiter})\n",
    "        candidate= candidate_agent(candidate_messages)\n",
    "\n",
    "        evaluator_system=evaluator_system.format(job_requiremet=i, candidate=candidate)\n",
    "        evaluation_messages[0]={'role': 'system','content':evaluator_system}\n",
    "        score=evaluator_agent(evaluation_messages)\n",
    "        scores+=score\n",
    "        QnA.append({'recruiter':recruiter, 'candidate':candidate, 'score':score})\n",
    "\n",
    "        print(\"Recruiter: \", recruiter)\n",
    "        print(\"Candidate: \", candidate)\n",
    "        print(score)\n",
    "        print('-------------------------------------------------------------------------------------------------------------------')\n",
    "        candidate_messages.pop(1)\n",
    "\n",
    "    return QnA, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recruiter:  Do you have a Bachelor's Degree in Statistics?\n",
      "Candidate:  No, I do not have a Bachelor's Degree in Statistics. My educational background includes a B.Sc. in Physics, Chemistry, and Mathematics from the University of Lucknow.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have a Bachelor's Degree in Applied Mathematics?\n",
      "Candidate:  No, I do not have a Bachelor's Degree in Applied Mathematics. My educational background includes a B.Sc. in Physics, Chemistry, and Mathematics from the University of Lucknow.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have a Bachelor's Degree in Computer Science?\n",
      "Candidate:  No, I do not have a Bachelor's Degree in Computer Science. My educational background includes a B.Sc. in Physics, Chemistry, and Mathematics from the University of Lucknow.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Are you currently a final course student in a related field?\n",
      "Candidate:  Yes, I am currently a final course student in a related field. I am pursuing my M.Sc. in Mathematics from the Indian Institute of Technology Jodhpur, with a graduation date of May 2023.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience in Python programming?\n",
      "Candidate:  Yes, I have experience in Python programming. It is mentioned in my resume under the Skills section that I am proficient in Python. Additionally, I have utilized Python in various projects mentioned in my resume, such as the X-ray Image Classification for Pneumonia Detection project, where I constructed a high-accuracy medical image classification model using advanced deep learning techniques with PyTorch and CNN. I have also developed the dbautomate Python package in the Python Database Automation Toolkit project, streamlining data upload, storage, and deletion. Furthermore, I have employed Python in the Movie Recommendation System project, enhancing recommendation system accuracy by integrating content-based filtering and collaborative filtering. Therefore, I have practical experience in Python programming as mentioned in my resume.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with SKLearn?\n",
      "Candidate:  Yes, I have experience working with Scikit-Learn as mentioned in my resume. I have used Scikit-Learn library for implementing machine learning algorithms and models during my academic courses and projects.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with XGBoost?\n",
      "Candidate:  Yes, I have experience working with XGBoost. During my internship at neuron.ai, I implemented and evaluated over 9 regression models, including XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and Linear Regression. Based on their performance over the training set, I selected the top 4 models, which included XGBoost. Additionally, I employed advanced machine learning algorithms to forecast insurance premium prices, achieving an R² score of 0.88 and a Mean Absolute Error (MAE) of 2446.15 units. Therefore, I have hands-on experience with XGBoost and its application in regression tasks.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with PyCaret?\n",
      "Candidate:  Sorry, I do not have experience working with PyCaret.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with Tensorflow?\n",
      "Candidate:  Yes, I have experience working with TensorFlow. In my previous project on Stock Price Prediction, I utilized TensorFlow for implementing a data-fetching mechanism and processing 10 years of historical stock price data. I also used TensorFlow for LSTM (Long Short-Term Memory) modeling, time series analysis, machine learning, optimization, and model evaluation. This resulted in obtaining a 119.57 RMSE score on the test dataset and forecasting the next 150 days.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with PyTorch?\n",
      "Candidate:  Yes, I have experience working with PyTorch. In my project titled \"X-ray Image Classification for Pneumonia Detection\", I utilized PyTorch along with other technologies such as Streamlit, Docker, AWS EC2, and S3 to construct a high-accuracy medical image classification model. This model achieved an accuracy of 88.33% on the test set.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have any experience working with Huggingface?\n",
      "Candidate:  No, I do not have any experience working with Huggingface.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with Rasa?\n",
      "Candidate:  Sorry, I do not have experience working with Rasa.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  What experience do you have in Natural Language Processing (NLP)?\n",
      "Candidate:  I have experience in Natural Language Processing (NLP) as part of my academic courses. I have taken courses in Machine Learning and Deep Learning, which cover NLP techniques. Additionally, I have worked on projects that involve NLP, such as the Movie Recommendation System project mentioned in my resume. In this project, I integrated content-based filtering and collaborative filtering techniques to optimize a dataset of movies. I also leveraged NLP techniques to enhance system efficiency and user satisfaction. However, my specific experience in NLP is limited to the projects and courses mentioned in my resume.\n",
      "0.8\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with large language models?\n",
      "Candidate:  Yes, I have experience working with large language models. During my internship at neuron.ai, I employed advanced machine learning algorithms to forecast insurance premium prices, achieving an R² score of 0.88 and a Mean Absolute Error (MAE) of 2446.15 units. I also implemented and evaluated over 9 regression models, including XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and Linear Regression. Additionally, I have experience with libraries such as PyTorch, TensorFlow, and openai, which are commonly used for working with large language models.\n",
      "0.8\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have a Master of Laws (LLM) degree?\n",
      "Candidate:  No, I do not have a Master of Laws (LLM) degree.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  What experience do you have in machine learning?\n",
      "Candidate:  I have experience in machine learning through my work as a Data Science Intern at neuron.ai. During my internship, I employed advanced machine learning algorithms to forecast insurance premium prices, achieving an R² score of 0.88 and a Mean Absolute Error (MAE) of 2446.15 units. I implemented and evaluated over 9 regression models, including XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and Linear Regression, and selected the top 4 models based on their performance over the training set. I also orchestrated 10+ experiments and data pipelines with DagsHub. Additionally, I have academic coursework in Machine Learning and Deep Learning.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience in deep learning?\n",
      "Candidate:  Yes, I have experience in deep learning. I have completed academic courses in Deep Learning and have also worked on a project called \"X-ray Image Classification for Pneumonia Detection\" where I constructed a high-accuracy medical image classification model using advanced deep learning techniques. This project involved the use of PyTorch, CNN, and Docker to achieve a prediction latency of 3 to 5 seconds.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with AWS?\n",
      "Candidate:  Yes, I have experience working with AWS. In my previous role as a Data Science Intern at neuron.ai, I deployed models on AWS EC2 and managed data pipelines using DVC on DagsHub. Additionally, I containerized applications with Docker and monitored models using MLFlow.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with Google Cloud Platform (GCP)?\n",
      "Candidate:  Based on the information provided in my resume, I do not have any experience working with Google Cloud Platform (GCP). My experience primarily lies in machine learning algorithms, deep learning techniques, data analysis, and software development using languages such as Python, C++, SQL, HTML, and CSS. I have also worked with various libraries and technologies such as PyTorch, TensorFlow, Docker, MySQL, AWS EC2, and Flask. However, GCP is not mentioned in my resume, indicating that I do not have direct experience with it.\n",
      "0.5\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with Python?\n",
      "Candidate:  Yes, I have experience working with Python. It is one of the languages mentioned in my skills section. I have used Python for various tasks, including implementing machine learning algorithms, developing data automation tools, and building a movie recommendation system. I have also utilized Python libraries such as PyTorch, TensorFlow, Pandas, Numpy, Matplotlib, and Scikit-Learn for data analysis and modeling. Additionally, I have used Python for web development using frameworks like Flask and for containerization with Docker.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have experience working with LLM or GenAI frameworks?\n",
      "Candidate:  No, I do not have any experience working with LLM or GenAI frameworks.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you explain the process of model fine-tuning?\n",
      "Candidate:  Yes, I can explain the process of model fine-tuning. In my previous role as a Data Science Intern at neuron.ai, I have experience with model fine-tuning. \n",
      "\n",
      "Model fine-tuning involves adjusting the parameters of a pre-trained model to improve its performance on a specific task or dataset. The process typically involves the following steps:\n",
      "\n",
      "1. Selecting a pre-trained model: Choose a pre-trained model that is relevant to the task at hand. For example, in my internship, I worked with various regression models such as XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and Linear Regression.\n",
      "\n",
      "2. Data preparation: Prepare the dataset by cleaning, preprocessing, and splitting it into training and validation sets. This step ensures that the data is in a suitable format for training and evaluation.\n",
      "\n",
      "3. Freezing the pre-trained layers: Freeze the weights of the pre-trained layers to prevent them from being updated during training. This is done to retain the knowledge learned by\n",
      "0.8\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you explain the process of model training and evaluation?\n",
      "Candidate:  In my previous role as a Data Science Intern at neuron.ai, I gained experience in model training and evaluation. During the project, I employed advanced machine learning algorithms to forecast insurance premium prices. To train the models, I implemented and evaluated over 9 regression models, including XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and Linear Regression. I selected the top 4 models based on their performance over the training set.\n",
      "\n",
      "To evaluate the models, I used the R² score and Mean Absolute Error (MAE). The trained models achieved an impressive R² score of 0.88 and a MAE of 2446.15 units. Additionally, I orchestrated 10+ experiments and data pipelines using DagsHub to streamline the process.\n",
      "\n",
      "Throughout the training and evaluation process, I monitored the models using MLFlow and managed the data pipelines with DVC on DagsHub. I also containerized the application with Docker and deployed it on AWS EC2, achieving\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you describe your experience in developing CI/CD pipelines?\n",
      "Candidate:  In my previous role as a Data Science Intern at neuron.ai, I had the opportunity to work on developing CI/CD pipelines. I utilized DagsHub to orchestrate 10+ experiments and data pipelines. I also managed data pipelines with DVC on DagsHub. Additionally, I containerized applications with Docker and deployed them on AWS EC2. These efforts resulted in achieving an inference time of 3 to 5 seconds. I also have experience with GitHub Action Server for CI/CD.\n",
      "0.8\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you explain your experience with containerizing models?\n",
      "Candidate:  In my previous role as a Data Science Intern at neuron.ai, I gained experience in containerizing models using Docker. I successfully containerized an application and deployed it on AWS EC2. This allowed for efficient management and deployment of the models. Additionally, I monitored the models using MLFlow and managed data pipelines with DVC on DagsHub. The containerization process helped achieve an inference time of 3 to 5 seconds, ensuring fast and reliable predictions.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you provide an example of a project where you used prototyping?\n",
      "Candidate:  I apologize, but there is no mention of using prototyping in any of the projects mentioned in my resume.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you provide an example of a POC (Proof of Concept) that you have developed in the past?\n",
      "Candidate:  Yes, I have developed a Proof of Concept (POC) for a Movie Recommendation System. The POC involved integrating 90% content-based filtering with 10% collaborative filtering to enhance the accuracy of the recommendation system. The dataset used for optimization consisted of over 10,000 IMDb and OMDb movies. The POC aimed to increase user retention by providing more accurate and personalized movie recommendations.\n",
      "\n",
      "To achieve this, I leveraged Flask, Azure, HTML, CSS, Bootstrap, JavaScript, and Cosine Similarity. The POC resulted in a recommendation inference latency of 5-7 seconds, improving system efficiency and user satisfaction.\n",
      "\n",
      "The POC demonstrated the effectiveness of the recommendation system by optimizing the dataset and integrating different filtering techniques. The use of Flask and other technologies ensured seamless integration and deployment of the system.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you explain your experience in designing and implementing architecture solutions?\n",
      "Candidate:  I apologize, but I do not have any experience mentioned in my resume related to designing and implementing architecture solutions.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have at least 1 year of professional experience with NLP and Large Language Models (LLM)?\n",
      "Candidate:  Based on the information provided in my resume, I do not have any professional experience specifically mentioned with NLP and Large Language Models (LLM).\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Do you have at least 2 years of experience in implementing and deploying machine learning and deep learning frameworks on AWS and GCP?\n",
      "Candidate:  Based on the information provided in my resume, I do not have at least 2 years of experience in implementing and deploying machine learning and deep learning frameworks on AWS and GCP.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you describe your experience working with the end-to-end machine learning project life cycle?\n",
      "Candidate:  In my role as a Data Science Intern at neuron.ai, I have gained experience working with the end-to-end machine learning project life cycle. I have implemented and evaluated various regression models, including XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and Linear Regression, to forecast insurance premium prices. I have achieved an R² score of 0.88 and a Mean Absolute Error (MAE) of 2446.15 units.\n",
      "\n",
      "I have also worked on projects such as X-ray Image Classification for Pneumonia Detection, where I constructed a high-accuracy medical image classification model using advanced deep learning techniques. Additionally, I developed the dbautomate Python package, streamlining data upload, storage, and deletion, resulting in a 50% reduction in data handling errors and increased efficiency for users.\n",
      "\n",
      "Throughout these projects, I have utilized various technologies and tools such as Docker, AWS EC2, S3, PyTorch, CNN, Flask, and GitHub\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you provide an example of a project where you worked with a team to deploy LLMs from start to finish?\n",
      "Candidate:  I apologize, but I do not have any experience mentioned in my resume related to working with a team to deploy LLMs (Language Model Models) from start to finish.\n",
      "0\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  What are some recent advancements in NLP and large language models that you find particularly interesting?\n",
      "Candidate:  Based on my resume, I have experience in NLP and have worked with libraries such as PyTorch and TensorFlow. However, my resume does not mention any specific recent advancements in NLP or large language models that I find particularly interesting. I apologize for not being able to provide a precise answer to your question.\n",
      "0.5\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  How have you applied novel techniques and methodologies to improve models in your previous work experience?\n",
      "Candidate:  In my previous work experience as a Data Science Intern at neuron.ai, I applied novel techniques and methodologies to improve models. I employed advanced machine learning algorithms to forecast insurance premium prices, achieving an R² score of 0.88 and a Mean Absolute Error (MAE) of 2446.15 units. I implemented and evaluated over 9 regression models, including XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and Linear Regression, and selected the top 4 models based on their performance over the training set. Additionally, I orchestrated 10+ experiments and data pipelines with DagsHub, monitored models using MLFlow, managed data pipelines with DVC on DagsHub, containerized the application with Docker, and deployed them on AWS EC2, achieving an inference time of 3 to 5 seconds. These techniques and methodologies allowed me to optimize model performance, streamline workflows, and enhance the overall efficiency of the project.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you describe your experience with testing, implementing, maintaining, tracking, and optimizing predictive models?\n",
      "Candidate:  As per my resume, I have experience in implementing and evaluating regression models for insurance premium price forecasting, achieving an R² score of 0.88 and a Mean Absolute Error (MAE) of 2446.15 units. I have also worked on advanced deep learning techniques for medical image classification, achieving 88.33% accuracy on the test set. Additionally, I have optimized recommendation systems by integrating content-based and collaborative filtering, resulting in increased user retention. I have also implemented a stock price prediction model using time series analysis, machine learning, and optimization techniques. \n",
      "\n",
      "In terms of maintaining and tracking models, I have used MLFlow to monitor models and managed data pipelines with DVC on DagsHub. I have also containerized applications with Docker and deployed them on AWS EC2, achieving an inference time of 3 to 5 seconds. \n",
      "\n",
      "Regarding optimization, I have utilized techniques such as XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and\n",
      "0.8\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you provide an example of a prototype or POC that you have designed in the past?\n",
      "Candidate:  Yes, I have designed a prototype called \"X-ray Image Classification for Pneumonia Detection\" as mentioned in my resume. This prototype involved constructing a high-accuracy medical image classification model using advanced deep learning techniques. The model achieved an accuracy of 88.33% on the test set. To enhance scalability and deployment efficiency, I adopted Streamlit, Docker, AWS EC2, S3, PyTorch, and CNN. This resulted in a prediction latency of 3 to 5 seconds. Additionally, I implemented CI/CD practices to streamline the development process. The prototype successfully demonstrated the potential of using deep learning algorithms for accurate pneumonia detection in X-ray images.\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Recruiter:  Can you describe your experience in supporting all stages of the ML model life cycle?\n",
      "Candidate:  In my role as a Data Science Intern at neuron.ai, I have gained experience in supporting all stages of the ML model life cycle. I have implemented and evaluated various regression models, including XGBoost, CatBoost, Random Forest, Gradient Boost, LightGBM, and Linear Regression. I have also employed advanced machine learning algorithms to forecast insurance premium prices, achieving an R² score of 0.88 and a Mean Absolute Error (MAE) of 2446.15 units.\n",
      "\n",
      "Furthermore, I have orchestrated multiple experiments and data pipelines using DagsHub, monitored models using MLFlow, and managed data pipelines with DVC on DagsHub. I have containerized applications with Docker and deployed them on AWS EC2, achieving an inference time of 3 to 5 seconds.\n",
      "\n",
      "Additionally, I have worked on projects such as X-ray Image Classification for Pneumonia Detection, where I constructed a high-accuracy medical image classification model using advanced deep learning techniques. I have also developed\n",
      "1\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "q, s=cal_score(job_requires, resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8040540540540541"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s/len(job_requires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5945945945945947"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s/len(job_requires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength=\"\"\n",
    "gaps=\"\"\n",
    "questions=\"\"\n",
    "for i in q:\n",
    "    if i['score']==0:\n",
    "        gaps+=i['recruiter']+'\\n'\n",
    "    elif i['score']==1:\n",
    "        strength+=i['recruiter']+'\\n'\n",
    "    else:\n",
    "        questions+=i['recruiter']+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gaps(messages):\n",
    "    res = openai_client.chat.completions.create(model=gpt_engine_name,\n",
    "                                                messages=messages,\n",
    "                                                temperature=0)\n",
    "    res=res.choices[0].message.content\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[{'role': 'system','content':f'''You will be provided with some points indicating the candidate's weaknesses. Based on these points, rephrase and list upto 5 most significant weaknesses. List them in a list and give a Json Response with weakness as key.\n",
    "weak points:\n",
    "{gaps}\n",
    "\n",
    "Output the Json Response with weakness as key and list of weaknesses as values.\n",
    "          \n",
    "Json Response:'''}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[{'role': 'system','content':f'''You will be provided with some points indicating the candidate's strengths. Based on these points, rephrase and list upto 5 most significant strengths. List them in a list and give a Json Response with strength as key.\n",
    "weak points:\n",
    "{strength}\n",
    "\n",
    "Output the Json Response with strength as key and list of strengths as values.\n",
    "          \n",
    "Json Response:'''}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "message=[{'role': 'system','content':f'''You will be provided with some points indicating the candidate's gaps. Based on these points, rephrase and list upto 5 questions that need to be asked to candidate to clarify them. List them in a list and give a Json Response with questions as key.\n",
    "weak points:\n",
    "{questions}\n",
    "\n",
    "Output the Json Response with strength as key and list of strengths as values.\n",
    "          \n",
    "Json Response:'''}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq=find_gaps(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"questions\": [\\n    \"What experience do you have in Natural Language Processing (NLP)?\",\\n    \"Do you have experience working with large language models?\",\\n    \"Do you have experience working with Google Cloud Platform (GCP)?\",\\n    \"Can you explain the process of model fine-tuning?\",\\n    \"Can you describe your experience in developing CI/CD pipelines?\"\\n  ]\\n}'"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=\"\"\n",
    "for i, j in json.loads(aq).items():\n",
    "    for k in j:\n",
    "        st+=k+'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What experience do you have in Natural Language Processing (NLP)?\n",
      "Do you have experience working with large language models?\n",
      "Do you have experience working with Google Cloud Platform (GCP)?\n",
      "Can you explain the process of model fine-tuning?\n",
      "Can you describe your experience in developing CI/CD pipelines?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item4': 1.0, 'item1': 0.9, 'item3': 0.75, 'item2': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Sample dictionary with scores\n",
    "scores = {\n",
    "    'item1': 0.9,\n",
    "    'item2': 0.5,\n",
    "    'item3': 0.75,\n",
    "    'item4': 1.0\n",
    "}\n",
    "\n",
    "# Sort the dictionary by values\n",
    "sorted_scores = dict(sorted(scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print the sorted dictionary\n",
    "print(sorted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item4\n",
      "item1\n",
      "item3\n",
      "item2\n"
     ]
    }
   ],
   "source": [
    "for i, j in sorted_scores.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraphNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.20 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.28 which is incompatible.\n",
      "langchain-openai 0.1.6 requires langchain-core<0.2.0,>=0.1.46, but you have langchain-core 0.2.28 which is incompatible.\n",
      "langchain-experimental 0.0.58 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.28 which is incompatible.\n",
      "langchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.2.28 which is incompatible.\n",
      "WARNING: You are using pip version 21.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading langgraph-0.1.19-py3-none-any.whl (102 kB)\n",
      "Collecting langchain-core<0.3,>=0.2.22\n",
      "  Downloading langchain_core-0.2.28-py3-none-any.whl (379 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (23.2)\n",
      "Collecting langsmith<0.2.0,>=0.1.75\n",
      "  Downloading langsmith-0.1.96-py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (4.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (8.3.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.3,>=0.2.22->langgraph) (2.7.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.22->langgraph) (2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langgraph) (2.32.3)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langgraph) (3.10.3)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.22->langgraph) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.22->langgraph) (0.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langgraph) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langgraph) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langgraph) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dudukuntasashidharre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.22->langgraph) (3.3.2)\n",
      "Installing collected packages: langsmith, langchain-core, langgraph\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.67\n",
      "    Uninstalling langsmith-0.1.67:\n",
      "      Successfully uninstalled langsmith-0.1.67\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.52\n",
      "    Uninstalling langchain-core-0.1.52:\n",
      "      Successfully uninstalled langchain-core-0.1.52\n",
      "Successfully installed langchain-core-0.2.28 langgraph-0.1.19 langsmith-0.1.96\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools for the agent to use\n",
    "@tool\n",
    "def user_approved_to_send_mail(email_content: str):\n",
    "    \"\"\"when user approves to send mai after asking for review\n",
    "     Input: content of the mail\"\"\"\n",
    "    # This is a placeholder, but don't tell the LLM that...\n",
    "    print(\"Email sent\")\n",
    "    return \"Email successfully sent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_email(state: MessagesState):\n",
    "#     \"\"\"Writes the email content .\"\"\"\n",
    "#     messages = state['messages']\n",
    "#     llm = AzureChatOpenAI(\n",
    "#     api_key=\"576b185ff87d4f50962560d349d06332\",\n",
    "#     azure_endpoint=\"https://securityappdewa.openai.azure.com/\",\n",
    "#     openai_api_version=\"2023-07-01-preview\",\n",
    "#     azure_deployment=\"gpt-35-turbo-16k\",\n",
    "#     temperature=0\n",
    "# )\n",
    "#     response = llm.invoke(messages)\n",
    "\n",
    "#     print(\"response\", response)\n",
    "#     # We return a list, because this will get added to the existing list\n",
    "#     return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_human_input(state: MessagesState):\n",
    "    \"Taking input from human\"\n",
    "    inp=input()\n",
    "    return {\"messages\": [HumanMessage(content=inp)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [user_approved_to_send_mail, take_human_input]\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "model = model = AzureChatOpenAI(\n",
    "    api_key=\"576b185ff87d4f50962560d349d06332\",\n",
    "    azure_endpoint=\"https://securityappdewa.openai.azure.com/\",\n",
    "    openai_api_version=\"2023-07-01-preview\",\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    temperature=0\n",
    ").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    print(\"tools\", last_message.tool_calls)\n",
    "    if last_message.tool_calls:\n",
    "        if last_message.tool_calls[0]['name']==\"user_approved_to_send_mail\":\n",
    "            print(\"in tools\")\n",
    "            return \"take_human_input\"\n",
    "        # Otherwise, we stop (reply to the user)\n",
    "        else:\n",
    "            return \"take_human_input\"\n",
    "    else:\n",
    "        return \"human_loop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    print('message', messages)\n",
    "    response = model.invoke(messages)\n",
    "    print(\"response\", response)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"human_loop\", take_human_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge('agent', \"human_loop\")\n",
    "workflow.add_edge('agent', \"tools\")\n",
    "\n",
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message [HumanMessage(content='write a mail to sashidhar for schedule interview', id='348f2264-6d24-4d46-934b-add129c21c9b')]\n",
      "response content='Sure, I can help you with that. Could you please provide me with the details of the interview, such as the date, time, and location?' response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 375, 'total_tokens': 407}, 'model_name': 'gpt-35-turbo-16k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-82623fde-f03c-4f49-8cc2-f98725ecc253-0'\n",
      "tools []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'human_loop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[379], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the Runnable\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwrite a mail to sashidhar for schedule interview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthread_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m final_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1281\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1283\u001b[0m     config,\n\u001b[0;32m   1284\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1285\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1286\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1287\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1288\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1290\u001b[0m ):\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1292\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:966\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[0;32m    965\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 966\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1367\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1366\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:2875\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2873\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2874\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2875\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2876\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[0;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\graph\\graph.py:82\u001b[0m, in \u001b[0;36mBranch._route\u001b[1;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[0;32m     80\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m     81\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minvoke(value, config)\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\graph\\graph.py:109\u001b[0m, in \u001b[0;36mBranch._finish\u001b[1;34m(self, writer, input, result)\u001b[0m\n\u001b[0;32m    107\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[1;32m--> 109\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends[r] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\graph\\graph.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    107\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[1;32m--> 109\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m result\n",
      "\u001b[1;31mKeyError\u001b[0m: 'human_loop'"
     ]
    }
   ],
   "source": [
    "# Use the Runnable\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"write a mail to sashidhar for schedule interview\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message [HumanMessage(content='write a mail to sashidhar for schedule interview', id='a4c32c83-e70a-4e25-a095-e087c08ec884'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mcGAR2VTgeXmAIsiW1cgYaE2', 'function': {'arguments': '{\\n  \"email_content\": \"Dear Sashidhar,\\\\n\\\\nI hope this email finds you well. I am writing to schedule an interview with you for the position you have applied for at our company.\\\\n\\\\nThe interview will be conducted on [date] at [time]. The interview will take place at our office located at [address].\\\\n\\\\nPlease confirm your availability for the interview by replying to this email or contacting me at [phone number].\\\\n\\\\nIf you have any questions or need any further information, please feel free to reach out to me.\\\\n\\\\nWe look forward to meeting you and discussing your qualifications further.\\\\n\\\\nBest regards,\\\\n[Your Name]\\\\n[Your Position]\\\\n[Company Name]\"\\n}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 65, 'total_tokens': 223}, 'model_name': 'gpt-35-turbo-16k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-81f90121-55aa-4e4c-8934-da8c4290e8d1-0', tool_calls=[{'name': 'search', 'args': {'email_content': 'Dear Sashidhar,\\n\\nI hope this email finds you well. I am writing to schedule an interview with you for the position you have applied for at our company.\\n\\nThe interview will be conducted on [date] at [time]. The interview will take place at our office located at [address].\\n\\nPlease confirm your availability for the interview by replying to this email or contacting me at [phone number].\\n\\nIf you have any questions or need any further information, please feel free to reach out to me.\\n\\nWe look forward to meeting you and discussing your qualifications further.\\n\\nBest regards,\\n[Your Name]\\n[Your Position]\\n[Company Name]'}, 'id': 'call_mcGAR2VTgeXmAIsiW1cgYaE2', 'type': 'tool_call'}]), ToolMessage(content='Email successfully sent', name='search', id='26881d53-cc52-4610-bf39-8c27a58d1090', tool_call_id='call_mcGAR2VTgeXmAIsiW1cgYaE2'), AIMessage(content='I have sent the email to Sashidhar to schedule the interview.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 231, 'total_tokens': 247}, 'model_name': 'gpt-35-turbo-16k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-ac502f73-eaa3-4038-bc86-38706da8914f-0'), HumanMessage(content='hi hello', id='6075a7e2-db1a-4dfb-8def-cd4b1dd7f484')]\n",
      "response content='Hello! How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 256, 'total_tokens': 266}, 'model_name': 'gpt-35-turbo-16k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-ff126636-6993-4514-a207-0c86f409b730-0'\n",
      "tools []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Runnable\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi hello\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "final_state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Invitation for Interview - [Company Name]\n",
      "\n",
      "Dear Sashidhar,\n",
      "\n",
      "I hope this email finds you well. I am writing to invite you for an interview for the position of [Job Title] at [Company Name]. We were impressed with your qualifications and experience, and we believe that you would be a valuable addition to our team.\n",
      "\n",
      "The interview will be conducted on [Date] at [Time]. The interview will take place at our office located at [Address]. Please make sure to arrive at least 10 minutes before the scheduled time.\n",
      "\n",
      "During the interview, we will discuss your skills, experience, and how they align with the requirements of the position. We will also provide you with more information about our company and the role you are applying for. The interview process may include a combination of technical assessments, behavioral questions, and a discussion about your career goals.\n",
      "\n",
      "To confirm your availability and schedule the interview, please reply to this email with your preferred date and time. If the provided date and time are not suitable for you, please let us know your availability so that we can find an alternative.\n",
      "\n",
      "If you have any questions or need any further information, please feel free to reach out to me. We are excited to meet you and learn more about your qualifications.\n",
      "\n",
      "Thank you for considering this opportunity. We look forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "[Your Position]\n",
      "[Company Name]\n",
      "[Contact Information]\n"
     ]
    }
   ],
   "source": [
    "print(final_state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message [HumanMessage(content='write a mail to sashidhar for schedule interview', id='5408552f-2b4e-46e0-851d-1324891bc437'), AIMessage(content='Subject: Interview Scheduling Request\\n\\nDear Sashidhar,\\n\\nI hope this email finds you well. I am reaching out to you on behalf of our company to schedule an interview for a position you have applied for.\\n\\nWe have reviewed your application and are impressed with your qualifications and experience. We believe you could be a great fit for the role and would like to invite you for an interview to further discuss your skills and potential contributions to our team.\\n\\nThe interview will be conducted on [date] at [time] via [video call/phone call/in-person]. Please let us know your availability during the following time slots:\\n\\n1. [Date and time]\\n2. [Date and time]\\n3. [Date and time]\\n\\nIf none of the above time slots work for you, please suggest an alternative time and we will do our best to accommodate it.\\n\\nTo confirm your availability or suggest an alternative time, please reply to this email or contact me directly at [your contact information].\\n\\nWe look forward to hearing from you and discussing your candidacy further.\\n\\nBest regards,\\n\\n[Your Name]\\n[Your Position]\\n[Company Name]\\n[Contact Information]', additional_kwargs={'tool_calls': [{'id': 'call_bfpfhbihnZkmCe1FLk2gbu8Z', 'function': {'arguments': '{\\n  \"email_content\": \"Subject: Interview Scheduling Request\\\\n\\\\nDear Sashidhar,\\\\n\\\\nI hope this email finds you well. I am reaching out to you on behalf of our company to schedule an interview for a position you have applied for.\\\\n\\\\nWe have reviewed your application and are impressed with your qualifications and experience. We believe you could be a great fit for the role and would like to invite you for an interview to further discuss your skills and potential contributions to our team.\\\\n\\\\nThe interview will be conducted on [date] at [time] via [video call/phone call/in-person]. Please let us know your availability during the following time slots:\\\\n\\\\n1. [Date and time]\\\\n2. [Date and time]\\\\n3. [Date and time]\\\\n\\\\nIf none of the above time slots work for you, please suggest an alternative time and we will do our best to accommodate it.\\\\n\\\\nTo confirm your availability or suggest an alternative time, please reply to this email or contact me directly at [your contact information].\\\\n\\\\nWe look forward to hearing from you and discussing your candidacy further.\\\\n\\\\nBest regards,\\\\n\\\\n[Your Name]\\\\n[Your Position]\\\\n[Company Name]\\\\n[Contact Information]\"\\n}', 'name': 'user_approved_to_send_mail'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 501, 'prompt_tokens': 73, 'total_tokens': 574}, 'model_name': 'gpt-35-turbo-16k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-0ee429e4-31be-44f9-908b-628ef6da9563-0', tool_calls=[{'name': 'user_approved_to_send_mail', 'args': {'email_content': 'Subject: Interview Scheduling Request\\n\\nDear Sashidhar,\\n\\nI hope this email finds you well. I am reaching out to you on behalf of our company to schedule an interview for a position you have applied for.\\n\\nWe have reviewed your application and are impressed with your qualifications and experience. We believe you could be a great fit for the role and would like to invite you for an interview to further discuss your skills and potential contributions to our team.\\n\\nThe interview will be conducted on [date] at [time] via [video call/phone call/in-person]. Please let us know your availability during the following time slots:\\n\\n1. [Date and time]\\n2. [Date and time]\\n3. [Date and time]\\n\\nIf none of the above time slots work for you, please suggest an alternative time and we will do our best to accommodate it.\\n\\nTo confirm your availability or suggest an alternative time, please reply to this email or contact me directly at [your contact information].\\n\\nWe look forward to hearing from you and discussing your candidacy further.\\n\\nBest regards,\\n\\n[Your Name]\\n[Your Position]\\n[Company Name]\\n[Contact Information]'}, 'id': 'call_bfpfhbihnZkmCe1FLk2gbu8Z', 'type': 'tool_call'}]), SystemMessage(content='Your task is to write a mail to candidate and ask for the review, if user says this mail looks good and asks to send then only send the mail.', id='dded955d-d69e-4f7c-a6c2-d91b59b91e6b'), HumanMessage(content='Hi', id='2b1fac02-4c2e-4b8e-b135-b6ba126f172a'), SystemMessage(content='Your task is to write a mail to candidate and ask for the review, if user says this mail looks good and asks to send then only send the mail.', id='a47d6dd9-7ef2-44da-980a-0ba285891fa5'), HumanMessage(content='write mail', id='af7f73a3-8ad5-4404-bf1f-d3cdb46a9d17')]\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_bfpfhbihnZkmCe1FLk2gbu8Z\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[277], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m inp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use the Runnable\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYour task is to write a mail to candidate and ask for the review, if user says this mail looks good and asks to send then only send the mail.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthread_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inp\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1281\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1283\u001b[0m     config,\n\u001b[0;32m   1284\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   1285\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   1286\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   1287\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   1288\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1290\u001b[0m ):\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1292\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:966\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[0;32m    965\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 966\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1367\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1366\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\executor.py:60\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\pregel\\retry.py:25\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     23\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:2873\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2869\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   2870\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2871\u001b[0m )\n\u001b[0;32m   2872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2873\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langgraph\\utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[0;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[268], line 5\u001b[0m, in \u001b[0;36mcall_model\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      3\u001b[0m messages \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, messages)\n\u001b[1;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# We return a list, because this will get added to the existing list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:5060\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5055\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5056\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5057\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5058\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5059\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   5061\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5062\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5063\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5064\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:274\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    271\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    273\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    275\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    276\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    277\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    278\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    279\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    282\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    283\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    284\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:714\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    708\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    712\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    713\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:571\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    570\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    572\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    573\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    575\u001b[0m ]\n\u001b[0;32m    576\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:561\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    560\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 561\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    562\u001b[0m                 m,\n\u001b[0;32m    563\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    564\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    565\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    566\u001b[0m             )\n\u001b[0;32m    567\u001b[0m         )\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:793\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 793\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    794\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:522\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    520\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    521\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 522\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\resources\\chat\\completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DudukuntaSashidharRe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_bfpfhbihnZkmCe1FLk2gbu8Z\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    inp=input()\n",
    "    # Use the Runnable\n",
    "    final_state = app.invoke(\n",
    "        {\"messages\": [SystemMessage(content=\"Your task is to write a mail to candidate and ask for the review, if user says this mail looks good and asks to send then only send the mail.\"),\n",
    "            HumanMessage(content=inp)]},\n",
    "        config={\"configurable\": {\"thread_id\": 42}}\n",
    "    )\n",
    "\n",
    "    print(final_state[\"messages\"][-1].content)\n",
    "    if inp=='send':\n",
    "        break\n",
    "    print(\"Does this email looks good? do you want to add anything?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI\n",
    "from config import api_key, azure_endpoint, api_version, gpt_engine_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Email_Agent:\n",
    "    def __init__(self, openai_type):\n",
    "        if openai_type==\"azure_openai\":\n",
    "            self.openai_client = AzureOpenAI(\n",
    "                    azure_endpoint = azure_endpoint,\n",
    "                    api_key=api_key,\n",
    "                    api_version= api_version\n",
    "                )\n",
    "        else:\n",
    "            self.openai_client = OpenAI(api_key=api_key)\n",
    "\n",
    "        self.messages=[]\n",
    "\n",
    "    def _email_prompt(self, email, name, company_name, job_title):\n",
    "        email_system=f'''Your task is to Write email to schedule an interview upon shortlist. Write a precise and short email based on the provided details. If user wants to add extra information then re write the email.\n",
    "The interview will be in online mode. Do not mention any date or timing in the email or any company contact number. In the closing of the email, include only the company name and omit any recruiter names.\n",
    "Details to write maile are below:\n",
    "Email: {email}\n",
    "Nmae: {name}\n",
    "company_name: {company_name}\n",
    "job_title: {job_title}'''\n",
    "        return email_system\n",
    "    \n",
    "    def _find_details(self, resume_text, job_description):\n",
    "        system_prompt=f'''Your are a resume parser. Your task is to extract the email id, name of candidate from the given resume and company name, job title from the given job description. Output the Json Response with name, email, company_name and job_title as keys and there respective values.\n",
    "\n",
    "Resume:\n",
    "```{resume_text}```\n",
    "\n",
    "Extract the candidate name, email id from the above resume and give it in name, email keys.\n",
    "\n",
    "Job Description:\n",
    "```{job_description}```\n",
    "\n",
    "Extract the company name, job title from the above job description and give it in company_name, job_title keys.\n",
    "\n",
    "Output a Json Response with name, email, company_name and job_title as keys and there respective values.\n",
    "Json Response:\n",
    "'''\n",
    "        messages=[{'role':'system', 'content':system_prompt}]\n",
    "        res = self.openai_client.chat.completions.create(model=gpt_engine_name,\n",
    "                                                    messages=messages,\n",
    "                                                    temperature=0)\n",
    "        res=json.loads(res.choices[0].message.content)\n",
    "        email=res['email']\n",
    "        name=res['name']\n",
    "        company_name=res['company_name']\n",
    "        job_title=res['job_title']\n",
    "        return email, name, company_name, job_title\n",
    "    \n",
    "    def agent(self, resume_text, job_description, user_message):\n",
    "        if user_message:\n",
    "            self.messages.append({'role':'user', 'content':user_message})\n",
    "        else:\n",
    "            email, name, company_name, job_title=self._find_details(resume_text, job_description)\n",
    "            email_system=self._email_prompt(email, name, company_name, job_title)\n",
    "            self.messages.append({'role':'system', 'content': email_system})\n",
    "        if user_message==\"send\":\n",
    "            return \"Email has been sucessfully send to the user\" \n",
    "        res = self.openai_client.chat.completions.create(model=gpt_engine_name,\n",
    "                                                    messages=self.messages,\n",
    "                                                    temperature=0)\n",
    "        result=res.choices[0].message.content                                         \n",
    "        self.messages.append({'role':'assistant', 'content':result})  \n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_writer=Email_Agent(\"azure_openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Ravi Kumar,\n",
      "\n",
      "We are pleased to inform you that you have been shortlisted for the position of LLM Engineer at Fynd. Congratulations on making it to the next round!\n",
      "\n",
      "We would like to schedule an interview with you to further discuss your qualifications and experience. The interview will be conducted online. Please let us know your availability for the interview so that we can arrange a suitable time.\n",
      "\n",
      "Looking forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "Fynd\n",
      "-------------------------\n",
      "\n",
      "Do you Like to add anything to the mail? If yes, please give us your input\n",
      "\n",
      "If you feel the email looks good, type 'send' to send it to the candidate.\n",
      "\n",
      "\n",
      "Dear Ravi Kumar,\n",
      "\n",
      "We are pleased to inform you that you have been shortlisted for the position of LLM Engineer at Fynd. Congratulations on making it to the next round!\n",
      "\n",
      "We would like to schedule an interview with you to further discuss your qualifications and experience. The interview will be conducted online. We have scheduled the interview for 6th August 2024. \n",
      "\n",
      "Please let us know if this date and time work for you. If not, kindly provide us with your availability so that we can arrange a suitable time.\n",
      "\n",
      "Looking forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "Fynd\n",
      "-------------------------\n",
      "\n",
      "Do you Like to add anything to the mail? If yes, please give us your input\n",
      "\n",
      "If you feel the email looks good, type 'send' to send it to the candidate.\n",
      "\n",
      "\n",
      "Dear Ravi Kumar,\n",
      "\n",
      "We are pleased to inform you that you have been shortlisted for the position of LLM Engineer at Fynd. Congratulations on making it to the next round!\n",
      "\n",
      "We would like to schedule an interview with you to further discuss your qualifications and experience. The interview will be conducted online. We have scheduled the interview for 6th August 2024 at 4:30 PM.\n",
      "\n",
      "Please confirm if this date and time work for you. If not, kindly provide us with your availability so that we can arrange a suitable time.\n",
      "\n",
      "Looking forward to hearing from you soon.\n",
      "\n",
      "Best regards,\n",
      "Fynd\n",
      "-------------------------\n",
      "\n",
      "Do you Like to add anything to the mail? If yes, please give us your input\n",
      "\n",
      "If you feel the email looks good, type 'send' to send it to the candidate.\n",
      "\n",
      "\n",
      "Email has been sucessfully send to the user\n",
      "-------------------------\n",
      "\n",
      "Do you Like to add anything to the mail? If yes, please give us your input\n",
      "\n",
      "If you feel the email looks good, type 'send' to send it to the candidate.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e=email_writer.agent(resume_text, job_description, '')\n",
    "print(e)\n",
    "print('-------------------------')\n",
    "print('\\nDo you Like to add anything to the mail? If yes, please give us your input')\n",
    "print(\"\\nIf you feel the email looks good, type 'send' to send it to the candidate.\\n\\n\")\n",
    "while True:\n",
    "    user_message=input()\n",
    "    e=email_writer.agent(resume_text, job_description, user_message)\n",
    "    print(e)\n",
    "    print('-------------------------')\n",
    "    if user_message==\"send\":\n",
    "        break\n",
    "    print('\\nDo you Like to add anything to the mail? If yes, please give us your input')\n",
    "    print(\"\\nIf you feel the email looks good, type 'send' to send it to the candidate.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
